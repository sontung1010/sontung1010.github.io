---
title: "Intelligent Navigation of Autonomous Maritime Robots (Work in Progress)"
excerpt: "<b>Skills:</b> ROS, ROS2, Simulation (Rviz, Gazebo), Sensor Integration (LIDAR, Camera, IMU), Machine Learning (PyTorch, TensorFlow), SLAM, Control Systems (PID), Computer Vision (OpenCV), Python, C++, Linux, Bash/Shell Scripting, Git, Debugger, Microcontroller, PWM, SolidWorks<br/><img src='/images/usv.png'>"
collection: portfolio
---


<div>
    <b>Supervisors:</b> <a href="https://fieldrobotics.engin.umich.edu/team" target="_blank">Prof. Katherine A. Skinner</a> and <a href="https://www.linkedin.com/in/elena-shrestha/" target="_blank">Dr. Elena Shrestha</a><br>
</div>
<br><br><br>


<div>
    <p>
        <b>Brief:</b> While reinforcement learning offers potential for continual learning and adaptability in complex scenarios, its application to real-world robotics faces significant challenges. Unlike in simulations, physical platforms struggle to collect a diverse corpus of training data due to critical safety risks and the inherent constraints of operating within a dynamic and partially observable environment. Our work draws inspiration from the human capability to fuse and exploit multiple sensing modalities, construct comprehensive models of how the world operates, and then leverage those models to adeptly navigate in challenging and often unpredictable environments. Here is an overview of how unmanned vehicles (ground, air, and surface) can exploit a world model constructed through multimodal perception, to learn near-optimal policies for guidance and control. A key aspect of the approach is learning from imagination in which the world model is used to simulate future imagined trajectories, enabling it to anticipate potential risks before encountering them in the real world. Our ongoing work and long-term vision is to evolve the traditional sense-plan-act framework into a more intuitive and cognitively inspired sense-imagine-act model. <a href="https://www.youtube.com/watch?v=Orte6Uv_mGU&t=987s" target="_blank">Dr. Elena Shrestha's presentation of our research.</a>

    </p>
</div>


<div>
    <p>
        <b>Role:</b> Graduate Research Assistant, <b>Field Robotics Group</b><br>
    </p>
</div>


<div>
    <p>
        <b>Contribution:</b><br>
    </p>
</div>

<div>
    <p>
        - Designed and programmed the URDF model for the Heron Unmanned Surface Vehicle (USV) by Clearpath Robotics and developed a “world” model in ROS2 and Gazebo Garden for the Marine Hydrodynamics Lab.<br>
        - Prepared the Heron USV for real-world testing by setting up battery, electrical, and mechanical systems; customized and 3D-printed parts to accommodate sensors; connected LIDAR, camera, IMU, indoor GPS, and odometry sensors via ROS for smooth data acquisition and integration.<br>
        - Developed and implemented an object avoidance algorithm using Python and C++, integrated it with the USV’s mechanical control systems, and refined it through rigorous testing in real-world and simulated environments.<br>
        - Conducted real-world testing, collected sensor data (LIDAR, camera, odometry, IMU, velocity), replicated the data in simulation, analyzed autonomous navigation performance by extracting LIDAR data and evaluating vehicle stability during “straight line” and “2 buoys” tests under varying wave conditions.<br>
        - Engineered sensor data fusion by aligning the laser/scan frame with the odometry/filtered transform frame, ensuring accurate transformations between critical frames and synchronizing timestamps for SLAM analysis in RViz using the Hector SLAM package.<br>
        - Designed a camera holder using SolidWorks, installed it on the Heron USV without interfering with the LIDAR scan, and streamed real-time camera data to RViz during real-world vehicle testing.<br>
        - Post-processing collected data into a format suitable and segmenting collected images for reinforcement learning training, training a U-Net model for image segmentation to improve the USV’s perception capabilities.<br>
        - Trying to configure the actual vehicle’s camera URDF in RViz and updating the training simulation’s camera’s URDF for enhanced precision in control and navigation tasks.<br>



    </p>
</div>
<div>
    <iframe width="420" height="315"
    src="https://youtube.com/embed/gSU9hapYx8s" 
    title="Heron"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
    </iframe>
    <center>
        Simulation: Autonomous Object Avoidance
    </center>
</div>
<br>
<div>
    <iframe width="420" height="315"
    src="https://youtube.com/embed/Aa0qp4Q8fms" 
    title="Heron"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
    </iframe>
    <center>
        Real-world test: Autonomous Object Avoidance
    </center>
</div>
<br>


<div>
    <p>
        [<a href="" target="_blank">GitHub</a>][<a href="" target="_blank">Publication</a>] 
    </p>
    <p>
        <b>Skills:</b> ROS, ROS2, Simulation (Rviz, Gazebo), Sensor Integration (LIDAR, Camera, IMU), SLAM, Machine Learning (PyTorch, TensorFlow), Control Systems (PID), Computer Vision (OpenCV), Python, C++, Linux, Bash/Shell Scripting, Git, Debugger, Microcontroller, PWM, SolidWorks<br>
        <b>Contributors' Acknowledgement:</b>
    </p>
</div>