---
title: "Multimodal Perception & Reinforcement Learning for UxV Guidance & Control (Work in Progress)"
excerpt: "<b>Skills:</b> ROS, ROS2, Simulation (Rviz, Gazebo), Sensor Integration (LIDAR, Camera, IMU), Machine Learning (PyTorch, TensorFlow), SLAM, Control Systems (PID), Computer Vision (OpenCV), Python, C++, Linux, Bash/Shell Scripting, Git, Debugger, Microcontroller, PWM, SolidWorks<br/><img src='/images/UxV/MBRL_.png'>"
collection: portfolio
---


<div>
    <b>Supervisors:</b> <a href="https://www.roahmlab.com/ram-personal" target="_blank">Prof. Ram Vasudevan</a>, <a href="https://fieldrobotics.engin.umich.edu/team" target="_blank">Prof. Katherine A. Skinner</a> and <a href="https://www.linkedin.com/in/elena-shrestha/" target="_blank">Dr. Elena Shrestha</a>
</div>
<br><br><br>


<div>
    <p>
        <b>Brief:</b> While reinforcement learning offers potential for continual learning and adaptability in complex scenarios, its application to real-world robotics faces significant challenges. Unlike in simulations, physical platforms struggle to collect a diverse corpus of training data due to critical safety risks and the inherent constraints of operating within a dynamic and partially observable environment. Our work draws inspiration from the human capability to fuse and exploit multiple sensing modalities, construct comprehensive models of how the world operates, and then leverage those models to adeptly navigate in challenging and often unpredictable environments. Here is an overview of how unmanned vehicles (ground, air, and surface) can exploit a world model constructed through multimodal perception, to learn near-optimal policies for guidance and control. A key aspect of the approach is learning from imagination in which the world model is used to simulate future imagined trajectories, enabling it to anticipate potential risks before encountering them in the real world. Our ongoing work and long-term vision is to evolve the traditional sense-plan-act framework into a more intuitive and cognitively inspired sense-imagine-act model. <a href="https://www.youtube.com/watch?v=Orte6Uv_mGU&t=987s" target="_blank">Dr. Elena Shrestha's presentation of our research.</a>

    </p>
</div>


<div>
    <p>
        <b>Role:</b> Research Assistant<br>
    </p>
</div>


<div>
    <p>
        <b>Contribution:</b><br>
    </p>
</div>
<div>
    <p>
        <u>For TD-Rex Autonomous Rover with ROAHM Lab</u><br>
        - Developed waypoint-follower algorithm for multi-agent experiments using the ‘cartographer_ros’ package for map building and localization, contributing to the reinforcement learning experiment.<br>
        - Accelerated the project's progression to the testing phase by building and programming embedded control systems on STM32 VESC and Jetson TX2 board's Linux environment for a secondary autonomous rover. Successfully configured a basic teleoperation controller, conducted lidar scans, calibrated IMUs, and implemented SLAM using the cartographer. Resolved critical issues with cartographer_ros frame transformations, enhancing system reliability and performance.<br>
        - Participated in optimizing the board’s performance using the UNet deep learning architecture to enhance the efficiency of semantic segmentation by reducing its callback duration by 5 times.<br>
    </p>
</div>

<br>
<div>
    <iframe width="420" height="315"
    src="https://youtube.com/embed/T87CyFyN1CY" 
    title="TD-Rex"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
    </iframe>
    <center>
        Remote control test
    </center>
</div>
<br>
<div>
    <iframe width="420" height="315"
    src="https://youtube.com/embed/Z6HuKvsSP88" 
    title="TD-Rex"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
    </iframe>
    <center>
        Remote control test on terrain
    </center>
</div>
<br>
<div>
    <iframe width="420" height="315"
    src="https://youtube.com/embed/gcGseiUzURE" 
    title="TD-Rex"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
    </iframe>
    <center>
        MBRL Autonomous Experiment
    </center>
</div>
<br>
<div>
    <iframe width="420" height="315"
    src="https://youtube.com/embed/IzxAyukVzYE" 
    title="TD-Rex"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
    </iframe>
    <center>
        Real-time SLAM on the UGV robot
    </center>
</div>
<br>

<div>
    <p>
        <u>For USV Autonomous Maritime Robots with UM Field Robotics Group</u> <br> 	
        - Programmed the Unmanned Surface Vehicle’s (USV) Unified Robot Description Format (URDF) model and the University of Michigan’s Marine Hydrodynamics Lab “world” model to establish the simulation environment using ROS2 and Gazebo Garden. This setup facilitated the testing of autonomous functions and the conduct of deep-learning research.<br>
        - Developed an object avoidance algorithm using Python and C++ and assessed the experiment's performance in both real-world and simulated environments to produce better tuning parameters for subsequent simulations.<br>
        - Conducted real-world tests to collect LIDAR, odometry, drive, velocity, and IMU data, which was then replicated in the simulation environment. Currently, efforts are focused on analyzing and post-processing the data using Python.<br>
    </p>
</div>
<div>
    <iframe width="420" height="315"
    src="https://youtube.com/embed/gSU9hapYx8s" 
    title="Heron"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
    </iframe>
    <center>
        Simulation: Autonomous Object Avoidance
    </center>
</div>
<br>
<div>
    <iframe width="420" height="315"
    src="https://youtube.com/embed/Aa0qp4Q8fms" 
    title="Heron"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen>
    </iframe>
    <center>
        Real-world test: Autonomous Object Avoidance
    </center>
</div>
<br>


<div>
    <p>
        [<a href="" target="_blank">GitHub</a>][<a href="" target="_blank">Publication</a>] 
    </p>
    <p>
        <b>Skills:</b> ROS, ROS2, Simulation (Rviz, Gazebo), Sensor Integration (LIDAR, Camera, IMU), SLAM, Machine Learning (PyTorch, TensorFlow), Control Systems (PID), Computer Vision (OpenCV), Python, C++, Linux, Bash/Shell Scripting, Git, Debugger, Microcontroller, PWM, SolidWorks<br>
        <b>Contributors' Acknowledgement:</b>
    </p>
</div>